import streamlit as st
import pandas as pd
import yaml
from fuzzywuzzy import fuzz
import streamlit as st
from crawler import crawl_blocktempo_news
from nlp_processor import process_dataframe
from news_updater import update_news_data
from analyzer_lda_topic import load_news_data, load_keyword_rules, update_keyword_rules_with_lda, save_keyword_rules

# è®€å–æ–°èè³‡æ–™
@st.cache_data
def load_news_data():
    df = pd.read_csv("news_raw.csv", parse_dates=["time"])
    return df

# è®€å–é—œéµå­—è¦å‰‡
@st.cache_data
def load_keyword_rules():
    with open("keyword_rules.yaml", encoding="utf-8") as f:
        rules = yaml.safe_load(f)
    return rules

# æ¨¡ç³Šæ¯”å°æ–‡å­—èˆ‡æ‰€æœ‰ä¸»é¡Œ + é—œéµå­—
def detect_topic(user_input, keyword_rules, threshold=60):
    best_match_topic = None
    best_score = 0

    for topic, rule in keyword_rules.items():
        all_possible = [topic] + rule['keywords']
        for kw in all_possible:
            score = fuzz.partial_ratio(user_input.lower(), kw.lower())
            if score > best_score and score > threshold:
                best_score = score
                best_match_topic = topic

    return best_match_topic


def retrieve_news_by_topic(news_df, topic, keyword_rules, top_n=5):
    """
    æ ¹æ“šå‚³å…¥ä¸»é¡Œèˆ‡ keyword_rules å°æ‡‰æ‰¾å‡ºæ–°èï¼š
    1. labels æ¬„ä½ç›´æ¥ match ä¸»é¡Œ
    2. title_cut æ¬„ä½æ˜¯å¦åŒ…å«æ­¤ä¸»é¡Œçš„ä»»ä¸€ keywordï¼ˆå­—è©ç´šï¼‰
    3. æ“´å±•è‡³ç›¸åŒ LDA topic çš„æ–°è
    """
    if topic == "æœ€æ–°æ¶ˆæ¯":
        matched_df = news_df.sort_values(by="time", ascending=False).head(top_n)
        return matched_df
    else:
        # ç›¸é—œé—œéµå­—
        keywords = keyword_rules.get(topic, {}).get("keywords", [])
        keywords = [topic] + keywords  # æŠŠä¸»é¡Œæœ¬èº«ä¹ŸåŠ å…¥æŸ¥æ‰¾é—œéµåˆ—è¡¨

        # Step 1: ä½¿ç”¨ labels ç²¾æº–åŒ¹é…
        label_matches = news_df[news_df['labels'].str.contains(topic, na=False, case=False)]
        # print(label_matches,"1")

        # Step 2: ä½¿ç”¨ title_cut åˆ†è©æ¯”å° 
        def has_any_keyword(row, keywords):
            words = str(row['title_cut']).split()
            return any(kw in words for kw in keywords)

        title_cut_matches = news_df[news_df.apply(lambda row: has_any_keyword(row, keywords), axis=1)]
        # print(title_cut_matches,"2")

        # é€²éšæœå°‹:lda topic æ“´å¤§æœå°‹ç›¸é—œå­—è©
        matched_df = pd.concat([label_matches, title_cut_matches])
        if matched_df.empty:
            matched_lda_topics = matched_df['lda_topic'].dropna().unique()
            if len(matched_lda_topics) > 0:
                lda_expansion_df = news_df[news_df['lda_topic'].isin(matched_lda_topics)]
                matched_df = pd.concat([matched_df, lda_expansion_df])
            # print(matched_df,"3")
        
        
        # Step 3: å»é‡ã€æ’åºã€å›å‚³
        matched_df = matched_df.drop_duplicates(subset=["title"])
        matched_df = matched_df.sort_values(by="time", ascending=False).head(top_n)
        return matched_df


#  == main ==
st.set_page_config(page_title="æ–°èå°è©±æ©Ÿå™¨äºº", page_icon="ğŸ“°")
st.title("ğŸ¤– å€å¡ŠéŠæ–°èå°è©±æ©Ÿå™¨äºº")
st.caption("è¼¸å…¥èˆ‡æ–°èä¸»é¡Œç›¸é—œçš„å•é¡Œï¼Œæˆ‘æœƒå¹«ä½ æ‰¾å‡ºæœ€æ–°ç›¸é—œæ–°èï¼")
st.caption("æ©Ÿå™¨äººæ‰€åƒè€ƒä¹‹æ–°èçš†ä¾†è‡ªå‹•é©…å‹•å€å¡ŠéŠæ–°èç¶²ç«™ (BlockTempo)ä¹‹å€å¡ŠéŠå•†æ¥­æ‡‰ç”¨é‡‘èå¸‚å ´")

# è¼‰å…¥è³‡æ–™
news_df = load_news_data()
keyword_rules = load_keyword_rules()

# ä½¿ç”¨è€…è¼¸å…¥
user_input = st.text_input("è«‹å•ä½ æƒ³äº†è§£ä»€éº¼å‘¢ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå‡æ¯æ–°èã€æ¯”ç‰¹å¹£åƒ¹æ ¼ã€é€šè†¨æ€éº¼äº†ï¼Ÿï¼‰")

if st.button("æŸ¥è©¢") or user_input:
    if not user_input.strip():
        st.warning("â—ï¸è«‹è¼¸å…¥ä¸€å€‹å•é¡Œ...")
    else:
        # è©¦åœ–æ‰¾å‡ºä¸»é¡Œ
        detected_topic = detect_topic(user_input, keyword_rules)
        
        if detected_topic:
            st.success(f"ğŸ¯ åµæ¸¬åˆ°ä¸»é¡Œ: **{detected_topic}**")
            related_news = filtered_news = retrieve_news_by_topic(news_df, detected_topic, keyword_rules)

            if not related_news.empty:
                st.markdown("ğŸ“° **ä»¥ä¸‹æ˜¯ç›¸é—œæ–°èï¼š**")
                for _, row in related_news.iterrows():
                    title = row['title']
                    time_str = row['time'].strftime('%Y-%m-%d') if pd.notna(row['time']) else "æœªçŸ¥"
                    url = row['url']
                    st.markdown(f"- [{title}]({url}) ({time_str})")
            else:
                st.text("ğŸ˜¢ æ‰¾ä¸åˆ°ç›¸é—œæ–°è")
        else:
            st.warning("â—ï¸ç„¡æ³•åˆ¤æ–·ä½ çš„æå•æ‰€å°æ‡‰çš„ä¸»é¡Œï¼Œè«‹æ›å€‹å•æ³•è©¦è©¦çœ‹ã€‚")

st.sidebar.title("ğŸ› ï¸ ç®¡ç†å·¥å…·")
if st.sidebar.button("ğŸ“¡ æ›´æ–°æ–°èè³‡æ–™"):
    with st.spinner("æ­£åœ¨çˆ¬å–æœ€æ–°æ–°è..."):
        raw_news = crawl_blocktempo_news()
        if not raw_news:
            st.warning("â›” æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–°èè³‡æ–™")
        else:
            st.success(f"å…±çˆ¬å– {len(raw_news)} ç¯‡æ–°èï¼Œæº–å‚™é€²è¡Œè™•ç†...")
            new_df = process_dataframe(pd.DataFrame(raw_news))
            updated_count = update_news_data(new_df)

            if updated_count == 0:
                st.info("âœ… æ²’æœ‰ç™¼ç¾æ–°æ–°è")
            else:
                st.success(f"âœ… æˆåŠŸæ–°å¢ {updated_count} å‰‡æ–°èè³‡æ–™ï¼")

                # é‡æ–°æ›´æ–°åˆ†é¡ rules
                with st.spinner("æ­£åœ¨æ›´æ–°ä¸»é¡Œå°æ‡‰çš„ LDA topic åˆ†é¡..."):
                    news_df = load_news_data()
                    keyword_rules = load_keyword_rules()
                    updated_rules = update_keyword_rules_with_lda(news_df, keyword_rules)
                    save_keyword_rules(updated_rules)
                    st.success("ğŸ”„ keyword_rules.yaml å·²è‡ªå‹•æ›´æ–° LDA topic!")

                # å¯é¸ï¼šé¡¯ç¤ºç°¡è¦
                with st.expander("ğŸ“‹ å…§å®¹é è¦½"):
                    st.dataframe(new_df.head())